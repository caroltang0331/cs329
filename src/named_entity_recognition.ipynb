{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dangerous-distance",
   "metadata": {},
   "source": [
    "\n",
    "NER with Gazetteer\n",
    "A gazetteer is a dictionary of lexicons indicating entity groups.\n",
    "Exercise\n",
    "Write the function recognize_ngram() that takes a sequence of tokens and a gazetteer and returns a list of entities where each entity is represented by a tuple consisting of the following 4 items:\n",
    "Index of the beginning token (inclusive)\n",
    "Index of the ending token (exclusive)\n",
    "Text span representing the entity (e.g., \"Emory University\")\n",
    "Set of named entity tags for the entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "built-classroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple, Set\n",
    "\n",
    "def recognize_ngram(tokens: List[str], gazetteer: Dict[str, Set[str]]) -> List[Tuple[int, int, str, Set[str]]]:\n",
    "    \"\"\"\n",
    "    :param tokens: a sequence of input tokens.\n",
    "    :param gazetteer: a dictionary whose key is the text span of a named entity (e.g., \"Emory University\") and the value is the set of named entity tags for the entity.\n",
    "    :return: a list of entities where each entity is represented by a tuple consisting of the following 4 items:\n",
    "             - Index of the beginning token (inclusive)\n",
    "             - Index of the ending token (exclusive)\n",
    "             - Text span representing the entity (e.g., \"Emory University\")\n",
    "             - Set of named entity tags for the entity\n",
    "    \"\"\"\n",
    "    entities = []\n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(i+1, len(tokens)+1):\n",
    "            key = ' '.join(tokens[i:j])\n",
    "            val = gazetteer.get(key, None)\n",
    "            if val: entities.append((i,j,key,val))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "peripheral-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAZETTEER = {\n",
    "    'Jinho': {'PER'},\n",
    "    'Jinho Choi': {'PER'},\n",
    "    'Emory': {'PER', 'ORG'},\n",
    "    'Emory University': {'ORG'},\n",
    "    'United States': {'GPE'},\n",
    "    'United States of America': {'GPE'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "organizational-summer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 'Jinho', {'PER'})\n",
      "(0, 2, 'Jinho Choi', {'PER'})\n",
      "(6, 7, 'Emory', {'PER', 'ORG'})\n",
      "(6, 8, 'Emory University', {'ORG'})\n",
      "(10, 12, 'United States', {'GPE'})\n",
      "(10, 14, 'United States of America', {'GPE'})\n"
     ]
    }
   ],
   "source": [
    "text = 'Jinho Choi is a professor at Emory University in the United States of America'\n",
    "tokens = text.split()\n",
    "\n",
    "entities = recognize_ngram(tokens, GAZETTEER)\n",
    "for entity in entities: print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-contract",
   "metadata": {},
   "source": [
    "\n",
    "NER using Prefix Tree\n",
    "The recgonize_ngram() function requires $O(n^2)$ search that can be very slow. In this case, using a more advanced data structure such as Trie (aka. prefix tree) can significantly reduce the search complexity. In the following example, we will use the Ahoâ€“Corasick Algorithm that is based on Trie but more efficient in saving millions of entries.\n",
    "Let us define a new gazetteer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "japanese-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAZETTEER = [\n",
    "    ('Jinho', 'PER'),\n",
    "    ('Jinho Choi', 'PER'),\n",
    "    ('Emory', 'PER'),\n",
    "    ('Emory', 'ORG'),\n",
    "    ('Emory University', 'ORG'),\n",
    "    ('United States', 'GPE'),\n",
    "    ('United States of America', 'GPE'),\n",
    "    ('Korean', 'LANG'),\n",
    "    ('Korea', 'GPE'),\n",
    "    ('South Korea', 'GPE'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lyric-retro",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyahocorasick in /Users/carol/PycharmProjects/cs329/venv/lib/python3.9/site-packages (1.4.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Users/carol/PycharmProjects/cs329/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyahocorasick\n",
    "import ahocorasick"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intimate-shaft",
   "metadata": {},
   "source": [
    "pyahocorasick\n",
    "\n",
    "We then write the create_ac() function to create the Aho-Corasick automation and add all entries in the gazetteer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "illegal-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple, Any\n",
    "from types import SimpleNamespace\n",
    "\n",
    "def create_ac(data: Iterable[Tuple[str, Any]]) -> ahocorasick.Automaton:\n",
    "    \"\"\"\n",
    "    Creates the Aho-Corasick automation and adds all (span, value) pairs in the data and finalizes this matcher.\n",
    "    :param data: a collection of (span, value) pairs.\n",
    "    \"\"\"\n",
    "    AC = ahocorasick.Automaton(ahocorasick.STORE_ANY)\n",
    "\n",
    "    for span, value in data:\n",
    "        if span in AC:\n",
    "            t = AC.get(span)\n",
    "        else:\n",
    "            t = SimpleNamespace(span=span, values=set())\n",
    "            AC.add_word(span, t)\n",
    "        t.values.add(value)\n",
    "\n",
    "    AC.make_automaton()\n",
    "    return AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "herbal-arrival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a = {'A':0, 'B':1, 'C':2}\n",
    "b = SimpleNamespace(A=0, B=1, C=2)\n",
    "\n",
    "print(a['A'])\n",
    "print(b.A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "compliant-making",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 namespace(span='Korea', values={'GPE'})\n",
      "19 namespace(span='Korean', values={'LANG'})\n",
      "47 namespace(span='United States', values={'GPE'})\n",
      "58 namespace(span='United States of America', values={'GPE'})\n"
     ]
    }
   ],
   "source": [
    "AC = create_ac(GAZETTEER)\n",
    "\n",
    "text = 'Dr. Choi is a Korean living in the United States of America '\n",
    "tokens = text.split()\n",
    "\n",
    "for eidx, t in AC.iter(text):\n",
    "    print(eidx, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-passage",
   "metadata": {},
   "source": [
    "Let us write the match() function that returns a list of entities where each entity spans over only token boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cooperative-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Set\n",
    "\n",
    "def match(AC: ahocorasick.Automaton, tokens: List[str]) -> List[Tuple[str, int, int, Set[str]]]:\n",
    "    \"\"\"\n",
    "    :param AC: the finalized Aho-Corasick automation.\n",
    "    :param tokens: the list of input tokens.\n",
    "    :return: a list of tuples where each tuple consists of\n",
    "             - span: str,\n",
    "             - start token index (inclusive): int\n",
    "             - end token index (exclusive): int\n",
    "             - a set of values for the span: Set[str]\n",
    "    \"\"\"\n",
    "    smap, emap, idx = dict(), dict(), 0\n",
    "    for i, token in enumerate(tokens):\n",
    "        smap[idx] = i\n",
    "        idx += len(token)\n",
    "        emap[idx] = i\n",
    "        idx += 1\n",
    "\n",
    "    # find matches\n",
    "    text = ' '.join(tokens)\n",
    "    spans = []\n",
    "    for eidx, t in AC.iter(text):\n",
    "        eidx += 1\n",
    "        sidx = eidx - len(t.span)\n",
    "        sidx = smap.get(sidx, None)\n",
    "        eidx = emap.get(eidx, None)\n",
    "        if sidx is None or eidx is None: continue\n",
    "        spans.append((t.span, sidx, eidx + 1, t.values))\n",
    "\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "continuous-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Korean', 4, 5, {'LANG'})\n",
      "('United States', 8, 10, {'GPE'})\n",
      "('United States of America', 8, 12, {'GPE'})\n"
     ]
    }
   ],
   "source": [
    "for span in match(AC, tokens):\n",
    "    print(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-malta",
   "metadata": {},
   "source": [
    "Write the function remove_subsets() that removes entities that are subsets of other entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fourth-boating",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_subsets(entities: List[Tuple[str, int, int, Set[str]]]) -> List[Tuple[str, int, int, Set[str]]]:\n",
    "    \"\"\"\n",
    "    :param entities: a list of tuples where each tuple consists of\n",
    "             - span: str,\n",
    "             - start token index (inclusive): int\n",
    "             - end token index (exclusive): int\n",
    "             - a set of values for the span: Set[str] \n",
    "    :return: a list of entities where each entity is represented by a tuple of (span, start index, end index, value set)\n",
    "    \"\"\"\n",
    "    tmp = []\n",
    "    # TODO: To be updated\n",
    "    for e0 in entities:\n",
    "        remove = False\n",
    "        for e1 in entities:\n",
    "            if e0 == e1: continue\n",
    "            if e0[1] >= e1[1] and e0[2] <= e1[2]:\n",
    "                remove = True\n",
    "                break\n",
    "        if not remove: tmp.append(e0)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stuffed-violation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Korean', 4, 5, {'LANG'})\n",
      "('United States of America', 8, 12, {'GPE'})\n"
     ]
    }
   ],
   "source": [
    "entities = match(AC, tokens)\n",
    "entities = remove_subsets(entities)\n",
    "for entity in entities: print(entity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "musical-basics",
   "metadata": {},
   "source": [
    "NER with Large Gazetteers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "requested-grain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "def read_gazetteers(dirname: str) -> ahocorasick.Automaton:\n",
    "    data = []\n",
    "    for filename in glob.glob(os.path.join(dirname, '*.txt')):\n",
    "        label = os.path.basename(filename)[:-4]\n",
    "        for line in open(filename):\n",
    "            data.append((line.strip(), label))\n",
    "    return create_ac(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "AC = read_gazetteers('../dat/ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-coach",
   "metadata": {},
   "outputs": [],
   "source": [
    "for span in match(AC, tokens):\n",
    "    print(span)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
