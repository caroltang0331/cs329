{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "innovative-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/carol/PycharmProjects/cs329/src\n",
      "/Users/carol/PycharmProjects/cs329 <class 'pathlib.PosixPath'>\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path = Path.cwd()\n",
    "print(path)\n",
    "\n",
    "while path.name != 'cs329':\n",
    "    path = path.parent\n",
    "\n",
    "print(path, type(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "confirmed-america",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/carol/PycharmProjects/cs329/dat/pos\n"
     ]
    }
   ],
   "source": [
    "path /= 'dat/pos'\n",
    "path.mkdir(parents=True, exist_ok=True)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "solved-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def download(remote_addr: str, local_addr: str):\n",
    "    r = requests.get(remote_addr)\n",
    "\n",
    "    with open(local_addr, 'wb') as fin:\n",
    "        fin.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fresh-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename: str):\n",
    "    data, sentence = [], []\n",
    "    fin = open(filename)\n",
    "    \n",
    "    for line in fin:\n",
    "        l = line.split()\n",
    "        if l:\n",
    "            sentence.append((l[0], l[1]))\n",
    "        else:\n",
    "            data.append(sentence)\n",
    "            sentence = []\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pharmaceutical-timeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38219\n",
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "trn_data = read_data(path / 'wsj-pos.trn.gold.tsv')\n",
    "print(len(trn_data))\n",
    "print(trn_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beginning-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "def word_count(data: List[List[Tuple[str, str]]]) -> int:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple list where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: the total number of words in the data\n",
    "    \"\"\"\n",
    "    return sum([len(sentence) for sentence in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "functioning-spain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912344\n"
     ]
    }
   ],
   "source": [
    "print(word_count(trn_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-publisher",
   "metadata": {},
   "source": [
    "predict: unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "binding-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "def create_uni_pos_dict(data: List[List[Tuple[str, str]]]) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is a word and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    model = dict()\n",
    "\n",
    "    for sentence in data:\n",
    "        for word, pos in sentence:\n",
    "            model.setdefault(word, Counter()).update([pos])\n",
    "\n",
    "    for word, counter in model.items():\n",
    "        ts = counter.most_common()\n",
    "        total = sum([count for _, count in ts])\n",
    "        model[word] = [(pos, count/total) for pos, count in ts]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "allied-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P(wi,wi-1,pi)/P(wi,wi-1)\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "def create_first_pos_dict(data: List[List[Tuple[str, str]]]) -> Dict[Tuple[str,str], List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is a tuple and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    PREV_DUMMY = '!@#$'\n",
    "    model = dict()\n",
    "    \n",
    "    for sentence in data:\n",
    "        for i, (word, pos) in enumerate(sentence):\n",
    "            prev_word = sentence[i-1][0] if i > 0 else PREV_DUMMY\n",
    "            model.setdefault((prev_word, word), Counter()).update([pos])\n",
    "    #print(model)\n",
    "    \n",
    "    for wordprevwordtuple, counter in model.items():\n",
    "        ts = counter.most_common()\n",
    "        total = sum([count for _, count in ts])\n",
    "        model[wordprevwordtuple] = [(pos, count/total) for pos, count in ts]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exceptional-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P(wi,wi+1,pi)/P(wi,wi+1)\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "def create_second_pos_dict(data: List[List[Tuple[str, str]]]) -> Dict[Tuple[str,str], List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is a tuple and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    NEXT_DUMMY = '!@#$'\n",
    "    model = dict()\n",
    "    \n",
    "    for sentence in data:\n",
    "        for i, (word, pos) in enumerate(sentence):\n",
    "            next_word = sentence[i+1][0] if i+1 < len(sentence) else NEXT_DUMMY\n",
    "            model.setdefault((word, next_word), Counter()).update([pos])\n",
    "    #print(model)\n",
    "    \n",
    "    for wordnextwordtuple, counter in model.items():\n",
    "        ts = counter.most_common()\n",
    "        total = sum([count for _, count in ts])\n",
    "        model[wordnextwordtuple] = [(pos, count/total) for pos, count in ts]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "convinced-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P(wi,pi-1,pi)/P(wi,pi-1)\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "def create_third_pos_dict(data: List[List[Tuple[str, str]]]) -> Dict[Tuple[str,str], List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is a tuple and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    PREV_DUMMY = '!@#$'\n",
    "    model = dict()\n",
    "    \n",
    "    for sentence in data:\n",
    "        for i, (word, pos) in enumerate(sentence):\n",
    "            prev_pos = sentence[i-1][1] if i > 0 else PREV_DUMMY\n",
    "            model.setdefault((prev_pos, word), Counter()).update([pos])\n",
    "    #print(model)\n",
    "    \n",
    "    for wordprevwordtuple, counter in model.items():\n",
    "        ts = counter.most_common()\n",
    "        total = sum([count for _, count in ts])\n",
    "        model[wordprevwordtuple] = [(pos, count/total) for pos, count in ts]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "smoking-action",
   "metadata": {},
   "outputs": [],
   "source": [
    "#P(pi-1,pi+1,pi)/P(pi-1,pi+1)\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "\n",
    "def create_fourth_pos_dict(data: List[List[Tuple[str, str]]]) -> Dict[Tuple[str,str], List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is a tuple and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    PREV_DUMMY = '!@#$'\n",
    "    NEXT_DUMMY = '!@#$'\n",
    "    model = dict()\n",
    "    \n",
    "    for sentence in data:\n",
    "        for i, (word, pos) in enumerate(sentence):\n",
    "            prev_pos = sentence[i-1][1] if i > 0 else PREV_DUMMY\n",
    "            next_pos = sentence[i+1][1] if i+1 < len(sentence) else NEXT_DUMMY\n",
    "            model.setdefault((prev_pos, next_pos), Counter()).update([pos])\n",
    "    #print(model)\n",
    "    \n",
    "    for wordprevwordtuple, counter in model.items():\n",
    "        ts = counter.most_common()\n",
    "        total = sum([count for _, count in ts])\n",
    "        model[wordprevwordtuple] = [(pos, count/total) for pos, count in ts]\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "elegant-baghdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('!@#$', 'JJ'): [('CD', 1.0)], ('CD', 'NN'): [('JJ', 0.5), ('NNP', 0.5)], ('JJ', 'NNP'): [('NN', 1.0)], ('NN', 'CD'): [('NNP', 1.0)], ('NNP', '!@#$'): [('CD', 0.5), ('NN', 0.5)], ('!@#$', 'NNP'): [('CD', 1.0)]}\n"
     ]
    }
   ],
   "source": [
    "result = dict()\n",
    "\n",
    "test = [[('Pierre', 'CD'),('nonexecutive', 'JJ'), ('director', 'NN'), ('nonexecutive', 'NNP'), ('director', 'CD')], [('Pierre', 'CD'),('nonexecutive', 'NNP'), ('director', 'NN')]]\n",
    "result = (create_fourth_pos_dict(test))\n",
    "print (result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-ranking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "south-germany",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_pos_dict = create_uni_pos_dict(trn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "musical-possibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('NN', 0.9714285714285714), ('VB', 0.01904761904761905), ('UH', 0.009523809523809525)]\n",
      "[('VB', 0.8293216630196937), ('VBP', 0.08971553610503283), ('NN', 0.06564551422319474), ('JJ', 0.015317286652078774)]\n"
     ]
    }
   ],
   "source": [
    "print(uni_pos_dict['man'])\n",
    "print(uni_pos_dict['buy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "hybrid-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_uni_pos_dict(uni_pos_dict: Dict[str, List[Tuple[str, float]]], tokens: List[str], pprint=False) -> List[Tuple[str, float]]:\n",
    "    def predict(token):\n",
    "        t = uni_pos_dict.get(token, None)\n",
    "        return t[0] if t else ('XX', 0.0)\n",
    "\n",
    "    output = [predict(token) for token in tokens]\n",
    "    if pprint:\n",
    "        for token, t in zip(tokens, output):\n",
    "            print('{:<15}{:<8}{:.2f}'.format(token, t[0], t[1]))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "suspended-logic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I              PRP     0.99\n",
      "bought         VBD     0.65\n",
      "a              DT      1.00\n",
      "car            NN      1.00\n",
      "yesterday      NN      0.98\n",
      "that           IN      0.60\n",
      "was            VBD     1.00\n",
      "blue           JJ      0.86\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('PRP', 0.9915824915824916),\n",
       " ('VBD', 0.6474820143884892),\n",
       " ('DT', 0.9987005955603682),\n",
       " ('NN', 1.0),\n",
       " ('NN', 0.9813432835820896),\n",
       " ('IN', 0.6039103975139195),\n",
       " ('VBD', 1.0),\n",
       " ('JJ', 0.8571428571428571)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = \"I bought a car yesterday that was blue\".split()\n",
    "predict_uni_pos_dict(uni_pos_dict, tokens, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "future-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_uni_pos(uni_pos_dict: Dict[str, List[Tuple[str, float]]], data: List[List[Tuple[str, str]]]):\n",
    "    total, correct = 0, 0\n",
    "    for sentence in data:\n",
    "        tokens, gold = tuple(zip(*sentence))\n",
    "        pred = [t[0] for t in predict_uni_pos_dict(uni_pos_dict, tokens)]\n",
    "        total += len(tokens)\n",
    "        correct += len([1 for g, p in zip(gold, pred) if g == p])\n",
    "    print('{:5.2f}% ({}/{})'.format(100.0 * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "sudden-arthur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.88% (119754/131768)\n"
     ]
    }
   ],
   "source": [
    "dev_data = read_data(path / 'wsj-pos.dev.gold.tsv')\n",
    "evaluate_uni_pos(uni_pos_dict, dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-mitchell",
   "metadata": {},
   "source": [
    "Bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adjustable-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "PREV_DUMMY = '!@#$'\n",
    "\n",
    "def to_probs(model: Dict[Any, Counter]):\n",
    "    for feature, counter in model.items():\n",
    "        ts = counter.most_common()\n",
    "        total = sum([count for _, count in ts])\n",
    "        model[feature] = [(pos, count/total) for pos, count in ts]\n",
    "    return model\n",
    "\n",
    "def create_bi_pos_dict(data: List[List[Tuple[str, str]]]) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is the previous POS tag and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    model = dict()\n",
    "\n",
    "    for sentence in data:\n",
    "        for i, (_, curr_pos) in enumerate(sentence):\n",
    "            prev_pos = sentence[i-1][1] if i > 0 else PREV_DUMMY\n",
    "            model.setdefault(prev_pos, Counter()).update([curr_pos])\n",
    "\n",
    "    return to_probs(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "underlying-dairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_pos_dict = create_bi_pos_dict(trn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "amended-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bi_pos_dict(uni_pos_dict: Dict[str, List[Tuple[str, float]]], bi_pos_dict: Dict[str, List[Tuple[str, float]]], tokens: List[str]) -> List[Tuple[str, float]]:\n",
    "    output = []\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        pos = uni_pos_dict.get(tokens[i], None)\n",
    "        if pos is None:\n",
    "            pos = bi_pos_dict.get(output[i-1][0] if i > 0 else PREV_DUMMY, None)\n",
    "        output.append(pos[0] if pos else ('XX', 0.0))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "threatened-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bi_pos(uni_pos_dict: Dict[str, List[Tuple[str, float]]], bi_pos_dict: Dict[str, List[Tuple[str, float]]], data: List[List[Tuple[str, str]]]):\n",
    "    total, correct = 0, 0\n",
    "    for sentence in data:\n",
    "        tokens, gold = tuple(zip(*sentence))\n",
    "        pred = [t[0] for t in predict_bi_pos_dict(uni_pos_dict, bi_pos_dict, tokens)]\n",
    "        total += len(tokens)\n",
    "        correct += len([1 for g, p in zip(gold, pred) if g == p])\n",
    "    print('{:5.2f}% ({}/{})'.format(100.0 * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "double-coordinator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.01% (121234/131768)\n"
     ]
    }
   ],
   "source": [
    "evaluate_bi_pos(uni_pos_dict, bi_pos_dict, dev_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "middle-harris",
   "metadata": {},
   "source": [
    "Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "superior-carbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bi_wp_dict(data: List[List[Tuple[str, str]]]) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is the previous word and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    model = dict()\n",
    "\n",
    "    for sentence in data:\n",
    "        for i, (_, curr_pos) in enumerate(sentence):\n",
    "            prev_word = sentence[i-1][0] if i > 0 else PREV_DUMMY\n",
    "            model.setdefault(prev_word, Counter()).update([curr_pos])\n",
    "\n",
    "    return to_probs(model)\n",
    "\n",
    "\n",
    "def create_bi_wn_dict(data: List[List[Tuple[str, str]]]) -> Dict[str, List[Tuple[str, float]]]:\n",
    "    \"\"\"\n",
    "    :param data: a list of tuple lists where each inner list represents a sentence and every tuple is a (word, pos) pair.\n",
    "    :return: a dictionary where the key is the previous word and the value is the list of possible POS tags with probabilities in descending order.\n",
    "    \"\"\"\n",
    "    model = dict()\n",
    "\n",
    "    for sentence in data:\n",
    "        for i, (_, curr_pos) in enumerate(sentence):\n",
    "            next_word = sentence[i+1][0] if i+1 < len(sentence) else PREV_DUMMY\n",
    "            model.setdefault(next_word, Counter()).update([curr_pos])\n",
    "\n",
    "    return to_probs(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "original-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_wp_dict = create_bi_wp_dict(trn_data)\n",
    "bi_wn_dict = create_bi_wn_dict(trn_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "satellite-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_interporlation(\n",
    "        uni_pos_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_pos_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_wp_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_wn_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        uni_pos_weight: float,\n",
    "        bi_pos_weight: float,\n",
    "        bi_wp_weight: float,\n",
    "        bi_wn_weight: float,\n",
    "        tokens: List[str]) -> List[Tuple[str, float]]:\n",
    "    output = []\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        scores = dict()\n",
    "        curr_word = tokens[i]\n",
    "        prev_pos = output[i-1][0] if i > 0 else PREV_DUMMY\n",
    "        prev_word = tokens[i-1] if i > 0 else PREV_DUMMY\n",
    "        next_word = tokens[i+1] if i+1 < len(tokens) else PREV_DUMMY\n",
    "\n",
    "        for pos, prob in uni_pos_dict.get(curr_word, dict()):\n",
    "            scores[pos] = scores.get(pos, 0) + prob * uni_pos_weight\n",
    "\n",
    "        for pos, prob in bi_pos_dict.get(prev_pos, dict()):\n",
    "            scores[pos] = scores.get(pos, 0) + prob * bi_pos_weight\n",
    "\n",
    "        for pos, prob in bi_wp_dict.get(prev_word, dict()):\n",
    "            scores[pos] = scores.get(pos, 0) + prob * bi_wp_weight\n",
    "\n",
    "        for pos, prob in bi_wn_dict.get(next_word, dict()):\n",
    "            scores[pos] = scores.get(pos, 0) + prob * bi_wn_weight\n",
    "\n",
    "        o = max(scores.items(), key=lambda t: t[1]) if scores else ('XX', 0.0)\n",
    "        output.append(o)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "digital-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_interpolation(\n",
    "        uni_pos_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_pos_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_wp_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        bi_wn_dict: Dict[str, List[Tuple[str, float]]],\n",
    "        uni_pos_weight: float,\n",
    "        bi_pos_weight: float,\n",
    "        bi_wp_weight: float,\n",
    "        bi_wn_weight: float,\n",
    "        data: List[List[Tuple[str, str]]],\n",
    "        pprint=False):\n",
    "    total, correct = 0, 0\n",
    "    for sentence in data:\n",
    "        tokens, gold = tuple(zip(*sentence))\n",
    "        pred = [t[0] for t in predict_interporlation(uni_pos_dict, bi_pos_dict, bi_wp_dict, bi_wn_dict, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight, tokens)]\n",
    "        total += len(tokens)\n",
    "        correct += len([1 for g, p in zip(gold, pred) if g == p])\n",
    "        \n",
    "    accuracy = 100.0 * correct / total\n",
    "    print('{:5.2f}% - uni_pos: {:3.1f}, bi_pos: {:3.1f}, bi_wp: {:3.1f}, bi_np: {:3.1f}'.format(accuracy, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "indirect-competition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.25129014631777"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_pos_weight = 1.0\n",
    "bi_pos_weight = 1.0\n",
    "bi_wp_weight = 1.0\n",
    "bi_wn_weight = 1.0\n",
    "evaluate_interpolation(uni_pos_dict, bi_pos_dict, bi_wp_dict, bi_wn_dict, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight, dev_data, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "statewide-rehabilitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "==========================================================\n",
      "Best : 93.41% - uni_pos: 0.5, bi_pos: 0.2, bi_wp: 0.2, bi_np: 0.2\n",
      "Worst: 73.46% - uni_pos: 0.2, bi_pos: 0.5, bi_wp: 0.5, bi_np: 0.5\n"
     ]
    }
   ],
   "source": [
    "#grid = [0.1, 0.5, 1.0]\n",
    "grid = [0.2, 0.5]\n",
    "best = (0, None)\n",
    "worst = (100, None)\n",
    "count = 0\n",
    "for uni_pos_weight in grid:\n",
    "    for bi_pos_weight in grid:\n",
    "        for bi_wp_weight in grid:\n",
    "            for bi_wn_weight in grid:\n",
    "                count = count + 1\n",
    "                acc = evaluate_interpolation(uni_pos_dict, bi_pos_dict, bi_wp_dict, bi_wn_dict, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight, dev_data)\n",
    "                if acc > best[0]: best = (acc, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight)\n",
    "                if acc < worst[0]: worst = (acc, uni_pos_weight, bi_pos_weight, bi_wp_weight, bi_wn_weight)\n",
    "print(count)\n",
    "print('==========================================================')\n",
    "print('Best : {:5.2f}% - uni_pos: {:3.1f}, bi_pos: {:3.1f}, bi_wp: {:3.1f}, bi_np: {:3.1f}'.format(*best))\n",
    "print('Worst: {:5.2f}% - uni_pos: {:3.1f}, bi_pos: {:3.1f}, bi_wp: {:3.1f}, bi_np: {:3.1f}'.format(*worst))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-recovery",
   "metadata": {},
   "source": [
    "NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "boxed-prevention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "exciting-brazil",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 61] Connection\n",
      "[nltk_data]     refused>\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno 61] Connection refused>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "wound-disposal",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/carol/nltk_data'\n    - '/Users/carol/PycharmProjects/cs329/venv/nltk_data'\n    - '/Users/carol/PycharmProjects/cs329/venv/share/nltk_data'\n    - '/Users/carol/PycharmProjects/cs329/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-6dbd81bbb355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I bought a car yesterday that was blue.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cs329/venv/lib/python3.9/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \"\"\"\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m     return [\n\u001b[1;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cs329/venv/lib/python3.9/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cs329/venv/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cs329/venv/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/cs329/venv/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/carol/nltk_data'\n    - '/Users/carol/PycharmProjects/cs329/venv/nltk_data'\n    - '/Users/carol/PycharmProjects/cs329/venv/share/nltk_data'\n    - '/Users/carol/PycharmProjects/cs329/venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(\"I bought a car yesterday that was blue.\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nltk(data: List[List[Tuple[str, str]]]):\n",
    "    total, correct = 0, 0\n",
    "    for sentence in data:\n",
    "        tokens, gold = tuple(zip(*sentence))\n",
    "        pred = [pos for token, pos in nltk.pos_tag(tokens)]\n",
    "        total += len(tokens)\n",
    "        correct += len([1 for g, p in zip(gold, pred) if g == p])\n",
    "    print('{:5.2f}% ({}/{})'.format(100.0 * correct / total, correct, total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_nltk(dev_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
